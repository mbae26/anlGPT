pubs.acs.org/JPCA

Article

Graph-Based Approaches for Predicting Solvation Energy in
Multiple Solvents: Open Datasets and Machine Learning Models
Logan Ward,* Naveen Dandu, Ben Blaiszik, Badri Narayanan, Rajeev S. Assary, Paul C. Redfern,
Ian Foster, and Larry A. Curtiss

Downloaded via ARGONNE NATL LABORATORY on January 11, 2023 at 16:49:20 (UTC).
See https://pubs.acs.org/sharingguidelines for options on how to legitimately share published articles.

Cite This: J. Phys. Chem. A 2021, 125, 5990−5998

ACCESS

Metrics & More

Read Online

Article Recommendations

sı Supporting Information
*

ABSTRACT: The solvation properties of molecules, often estimated using quantum
chemical simulations, are important in the synthesis of energy storage materials,
drugs, and industrial chemicals. Here, we develop machine learning models of
solvation energies to replace expensive quantum chemistry calculations with
inexpensive-to-compute message-passing neural network models that require only
the molecular graph as inputs. Our models are trained on a new database of solvation
energies for 130,258 molecules taken from the QM9 dataset computed in ﬁve
solvents (acetone, ethanol, acetonitrile, dimethyl sulfoxide, and water) via an implicit
solvent model. Our best model achieves a mean absolute error of 0.5 kcal/mol for molecules with nine or fewer non-hydrogen atoms
and 1 kcal/mol for molecules with between 10 and 14 non-hydrogen atoms. We make the entire dataset of 651,290 computed
entries openly available and provide simple web and programmatic interfaces to enable others to run our solvation energy model on
new molecules. This model calculates the solvation energies for molecules using only the SMILES string and also provides an
estimate of whether each molecule is within the domain of applicability of our model. We envision that the dataset and models will
provide the functionality needed for the rapid screening of large chemical spaces to discover improved molecules for many
applications.

■

including convolutional neural networks9,10 and ﬁngerprintbased ML.11,12 Similar experimental databases, like those from
the University of Minnesota,13 provide solvation data for many
solvents and have also been used as the basis for ML models,
as illustrated by pioneering work from Borhani et al.14 and
Subramanian et al.15 The limited size and chemical diversity of
experimental data (only a few hundred molecules) make them
impractical to use with high-accuracy, deep learning models
(e.g., refs 16−18). This issue can be addressed by developing
large and diverse datasets of solvation properties (>100,000
molecules) via computations that provide suﬃcient representation of diﬀerent regions of the chemical space.
Here, we report on a new large dataset, QM9-solvation, of
computed solvation energies (ΔGsolv) for 130,258 molecules in
multiple solvents and provide an initial set of ML models
trained using this data. The dataset was generated from density
functional theory (DFT) simulations using the SMD solvation
model19 and is openly available for use by the chemistry
community.20 We also demonstrate a state-of-the-art ML
prediction model of molecular solvation energies trained on

INTRODUCTION
The ability of a molecule to dissolve in a particular solvent is a
key factor in determining whether it will be suitable for many
technological applications, such as drug molecules that must
dissolve in the bloodstream or redox-active molecules that
must remain in solution inside a ﬂow battery. Chemists assess
the solubility of a molecule by computing the solvation energy
(ΔGsolv)a measure of the strength of the interaction between
a molecule and a solventfrom quantum mechanical
simulations.1 While methods for computing solvation energy
are well established,2 their computational expense is large
enough (minutes to hours per compound on a single
processor) to impose serious limitations on how many
molecules can be assessed. Evaluating thousands of chemicals
requires signiﬁcant computational resources, yet would explore
a small fraction of modern chemical search spaces that
approach billions of molecules.3−6 Faster approaches, such as
machine-learned QSAR models,7 are required to reach the
necessary evaluation throughput to scan for promising leads in
these large chemical spaces.
The quality of a machine-learned model of a chemical
property is highly dependent on the size, breadth, and quality
of the available data. Existing machine learning (ML) models
for solvation energy have been trained using small databases of
experimental data, which limits their applicability. One such
database, FreeSolv,8 has been used extensively to create ML
models for hydrogenation energy based on diverse techniques
© 2021 UChicago Argonne, LLC,
Operator of Argonne National
Laboratory. Published by American
Chemical Society

Received: March 4, 2021
Revised: June 15, 2021
Published: June 30, 2021

5990

https://doi.org/10.1021/acs.jpca.1c01960
J. Phys. Chem. A 2021, 125, 5990−5998



The Journal of Physical Chemistry A

pubs.acs.org/JPCA

Article

Figure 1. Two MPNN versions used in this work: (a) a model that computes solvation energy based on a molecular ﬁngerprint and (b) a model
that computes solvation energy by summing contributions of each atom in a molecule. Boxes represent key computations in a network, and open
yellow arrows illustrate how data is passed between them. Orange closed arrows indicate the model outputs that represent molecular ﬁngerprints
for (a) and atomic ﬁngerprints for (b).

and nodes, and a function then updates the features of each
node based on its message. The combined “message-thenupdate” cycle can be repeated many times before the ﬁnal
“readout” function that produces a single feature vector for the
entire graph. The features for the graph are then passed as
inputs to another model (e.g., a fully connected neural
network) that maps the graph features to a property of the
graph. As with all neural networks, the weights of an MPNN
are learned by adjusting them to minimize the error between
prediction and true values through gradient descent.
We start with the message, update, and readout functions
used by St. John et al.30 The initial features of the atoms and
bonds are determined by looking up a set of features for each
type of atomdeﬁned using its element, number of bonds,
number of bonded hydrogen atoms, and whether it is
aromaticand for each bonddeﬁned using its type (e.g.,
single vs double), whether the bond is conjugated or in a ring,
and the elements of the bonded atoms. The features for each
type of atom or bond are learned while training the ML model.
The messages are produced by summing over the product of
atom and bond features of each neighboring atom. The
features are updated using a gated recurrent unit module,
which generates the new atom features considering both the
new message and all previous feature values for that atom. We
apply the message and update functions six times. After these
layers, we generate a set of features for each atom that reﬂect
its properties and those of up to the sixth-nearest neighbors.
As shown in Figure 1, we use two versions of the MPNN
that diﬀer based on whether the readout function is applied
before or after reducing the atomic features to a single scalar.
The ﬁrst, “molecular ﬁngerprint,” version uses a sum readout
function before reducing the atomic features to a single value,
which eﬀectively produces a set of features that describes the
entire molecule. The “atomic contribution” version reduces the
atomic features to a single scalar value before summing over all
atoms, which approximates each atom having an additive
contribution to solvation energy.
Full details of the models are available in the Supporting
Information on the GitHub page associated with this work and
on the Materials Data Facility.20,21

this database. Our models use a message-passing architecture
that includes the properties of both the solvent and the solute,
which we demonstrate is capable of learning solvent/solute
relationships from over a half-million training entries. We
establish a domain of applicability region for these models and
ﬁnd that it is possible to predict solvation energies of molecules
larger than those in our training set. The models are freely
available through GitHub and DLHub.21,22 We envision that
our database and models will help spur rapid advancements in
the ability for scientists to eﬃciently assess the solvation
properties of molecules.

■

METHODS
Our work involved ﬁrst creating a database of solvation
energies using quantum chemistry computations and creating
models to predict solvation energy using ML. We describe the
quantum chemistry and ML aspects of our work separately.
Quantum Chemistry. We performed DFT calculations
using Gaussian 16 software to compute solvation energies of
molecules.23 All calculations were performed using the B3LYP
functional and the 6-31G(2df,p) basis set starting with the
relaxed geometries from the QM924 and G4MP2-GDB9
datasets.25−27 Each relaxed geometry was then used in
computing single-point energies in the presence of ﬁve
diﬀerent solvents using a solvation model based on density
(SMD model)19 at the same level of theory that was used for
geometry relaxation. The solvents chosen for the DFT
calculations are acetone, ethanol, acetonitrile (ACN), dimethyl
sulfoxide, and water. The energies of these molecules in the gas
phase [Egas] and in the solvent [Esolvent] were used to calculate
the solvation energy [ΔGsol] of the molecules in each of the
solvents via the following equation.
[ΔGsol ] = [Esolvent ] − [Egas]

(1)

The computed solvation energies of all molecules from QM9
and selected molecules from the Pedley compilation,28
reported in kcal/mol, are available on the Materials Data
Facility.20
Machine Learning. We use message passing neural
networks (MPNNs) for most ML tasks described in this
work. As formalized by Gilmer et al.,17 MPNNs are a general
class of architectures for learning properties of graphs, and
there exist signiﬁcant degrees of freedom within the MPNN
design. The general architecture of MPNNs includes
“message” layers that produce a signal for each node (or, in
some architectures, each edge29), given its neighboring edges

■

RESULTS AND DISCUSSION
We describe the development of a ML model that predicts the
solvation energies of a diverse class of molecules in ﬁve
solvents. The following sections describe how we accomplished this goal by ﬁrst establishing a database of solvation
5991

https://doi.org/10.1021/acs.jpca.1c01960
J. Phys. Chem. A 2021, 125, 5990−5998



The Journal of Physical Chemistry A

pubs.acs.org/JPCA

Article

architectures nor to determine early stopping criteria employed
in the neural network training.
We use a set of 191 molecules with more than nine heavy
atoms to provide further evaluation of the models. These
molecules were selected in a previous study,27 where we
identiﬁed molecules with between 10 and 14 heavy atoms from
the Pedley Compilation28 that have accurate measurements of
formation enthalpy. These molecules capture molecular motifs
that are impossible to form with fewer than 10 heavy atoms,
such as polycyclic aromatic compounds. We denote these
molecules our “Pedley test set.” They were not selected
randomly but drawn from a set of molecules for which
solvation energies have been measured experimentally (unlike
those in the QM9 training set, which were generated
procedurally) and, further, were chosen to exhibit qualities
desirable for validation (e.g., possessing molecular motifs not
included in the QM9-solvation dataset).
The full dataset, which we denote QM9-solvation, is
available through GitHub and the Materials Data Facility.20,21
Predicting Solvation Energies of Molecules in Water
Solvent: MPNNs. Our goal is to link the graph structure of a
molecule to solvation energy. Using only the molecular graph
means that estimating the solvation energy of a molecule does
not require any force ﬁeld or quantum chemistry computations, which both allows our models to screen new chemicals at
high rates and mitigates concerns about tying predicted
solvation energy to a speciﬁc conformation.
The use of ML to link molecular structures to properties is
well studied.7 We choose to evaluate two techniques from the
literature due to their simplicity: group contribution with linear
regression and MPNNs. Both methods start by labeling atoms
in the molecule with type information, such as by element or
by functional group, but diﬀer signiﬁcantly in how these labels
are mapped to functional properties.
We modeled our group contribution approach on the widely
used model for water−octanol partition coeﬃcient (log P) of
Wildman and Crippen.38 We deﬁne functional groups for each
non-hydrogen atom in a molecule by ﬁrst assigning a type
based on the element of that atom, whether it is in an aromatic
cycle, and its degree. We then complete the description of the
atom by computing the types of the neighboring atoms using
the same procedure and recording the type of bonding. Our
procedure lacks the hand-tuned groups of Wildman and
Crippen but distinguishes atomic sites more fully. For example,
our method describes primary and secondary aliphatic carbons
diﬀerently; these are treated as identical by Wildman and
Crippen. We believe that this further granularity is needed for a
dataset as diverse as QM9. We represent each molecule as a
vector in which each entry corresponds to the count of that
functional group type. We then use L1 regularized linear
regression using the least absolute shrinkage and selection
operator (LASSO)39 to ﬁt a linear model between this vector
and solvation energy.
Our second type of model is a message-passing neural
network, which provides an automatic path for learning the
similarity between diﬀerent functional groups and their impact
on a material property. An MPNN, as formalized by Gilmer et
al.17 and described in the Methods section, learns interactions
between atoms/functional groups through “message-passing”
steps that update the description of each atom based on the
types of neighboring atoms and the types of bonds connecting
to the neighboring atoms. In this work, we use an MPNN
architecture implemented by St. John et al.30 Our MPNN

energies based on an unbiased sampling of molecular
structures and then gradually increasing the complexity of
ML techniques used to learn correlations from this data.
Database of 650,000 Solvation Energies of Small
Molecules. We base our training set on the QM9 database of
Ramakrishnan et al.24, which has been used frequently in the
molecular ML community for a number of reasons. First, the
database is large: with over 105 entries, QM9 provides
suﬃcient training data to support the development of many
ML models.29,31,32 Second, the database is diverse. QM9 is
based on an exhaustive sampling of molecules,5,33 which allows
us to train and validate our molecules with a minimal concern
over whether it is learning patterns related to the sampling of
data rather than the physics of the problem.
We used the SMD implicit solvation model in Gaussian to
compute the solvation energy for 130,258 molecules from
QM9. We selected SMD for its comparable accuracy to explicit
solvent models and reasonable correlation with experimental
solvation energies at a reduced computational cost.34,35 For
each molecule, we computed the solvation energy in ﬁve
solvents that sample a broad range of dielectric constants:
acetone (ϵ = 20.49), ethanol (ϵ = 24.85), acetonitrile (ACN)
(ϵ = 35.69), dimethyl sulfoxide (DMSO) (ϵ = 46.83), and
water (ϵ = 78.36). We selected these solvents to not only
capture diﬀerences among the solvation behavior of diﬀerent
molecules but also selecting solvents of technological relevance
(e.g., ACN and water are common solvents for ﬂow
batteries36).
As illustrated in Figure 2, the solvation energies for our
molecules follow, generally, a single-modal distribution. The

Figure 2. Histogram of solvation energies, ΔGsolv, of all 130,258
molecules in our QM9-solvation dataset in each of the ﬁve solvents
considered in this work.

notable exception is water, which features multiple peaks near
−10, −5, and 0 kcal/mol. The standard deviation in the
solvation energy is approximately 2.8 kcal/mol for all solvents
except water, which shows a wider distribution with a standard
deviation of 4.3 kcal/mol. Some molecules have solvation
energies substantially higher or lower than most in the dataset.
The solvation energies for these molecules can range from as
low as −84 kcal/mol to as high as 5 kcal/mol. For example,
there are 543 (0.4%) and 428 (0.3%) molecules with solvation
energies below −20 kcal/mol in water and acetone,
respectively.
We divide the QM9 molecules into a ﬁrst subset of 117,232
molecules, used for model training, and a second subset of
13,026 molecules, used for model evaluation. Following our
previous work,37 the latter 10% - drawn from the same
population as the training examples (i.e., molecules of smaller
than nine heavy atoms) - serve as a test set. The test set
molecules are neither used to train the parameters of any of the
5992

https://doi.org/10.1021/acs.jpca.1c01960
J. Phys. Chem. A 2021, 125, 5990−5998



The Journal of Physical Chemistry A

pubs.acs.org/JPCA

Article

solvation energies, with 15 of the top 25 errors being within
the 1% most negative solvation energies. The relatively poor
predictive performance on these materials is partly explained
by the solvation energy being outliers compared to the rest of
the data. Twelve of the top 25 (48%) errors belong to a class of
molecules, zwitterions, that is rare, with only 396 (0.3%)
entries within the training data. We conclude that the model’s
predictions are most reliable for molecules with solvation
energies of less negative than −25 kcal/mol and are, for
example, inaccurate for zwitterions.
Extending an MPNN Solvation Model to Other
Solvents. We next enhanced our MPNN model to predict
solvation energies in other solvents. We explored two diﬀerent
strategies for creating “multi-task” models able to predict
solvation energy in diﬀerent solvents simultaneously. The ﬁrst
approach is a standard multi-task model in which the last layer
of the neural network produces multiple outputs, one per
solvent. The second approach involves a network that predicts
solvation energy given the molecular structure of the solute
and the dielectric constant of the solvent. This “dielectric
constant” model, unlike the multi-task model, can predict
solvation energy in solvents not included in the training set.
Here, we test which method produces more accurate models
and how well the dielectric constant model computes solvation
energy for solvents outside of the training set.
We evaluate our two strategies by training a model with the
same 90/10% test split as our single-solvent model and data
from all the ﬁve solvents. The multi-task and dielectricconstant models have comparable accuracy to the singlesolvent model in predicting solvation energy in water. The
single-solvent model has an error of 0.44 kcal/mol for solvation
energy in watereﬀectively equal to the errors of 0.43 and
0.45 kcal/mol for the multi-task and dielectric constant
models, respectively. We note that we previously observed
that training a multi-task model with greatly varied properties
(e.g., highest occupied molecular orbital energy and atomization energy) can lead to a signiﬁcant degradation in model
accuracy.37 It appears that the solvation energies of a molecule
in diﬀerent solvents are suﬃciently similar such that modeling
diﬀerent solvents simultaneously in the same model does not
lead to degradation in performance.
We further explored the concept of multi-solvent prediction
by withholding data from one of our ﬁve solvents, ACN, to use
as a test set. We selected ACN for this test case because it has
the median dielectric constant of the ﬁve solvents and thus
should be a good test for our ML model’s ability to interpolate.

describes atoms initially based on their type, degree, number of
bonded hydrogen atoms, and whether it is in an aromatic ring;
updates the atom description with messages based on sums of
those from its nearby atoms; and generates a description of the
entire molecule as a sum of all atoms. Full details are available
in the Methods section.
We compared the MPNN and linear regression models for
predicting the solvation energy in water using the training and
test datasets described in the previous section. As shown in
Figure 3, our MPNN model has a mean absolute error (MAE)

Figure 3. Comparison of the performance of diﬀerent ML models on
predicting the solvation energy in water of 13,026 molecules outside
of their training set. Each model was trained on the same 117,232
molecules. From left to right: performance of group contribution
model trained using sparse linear regression (LASSO); MPNN
performance; and LASSO and MPNN vs training set size. LASSO fails
to continue improving in accuracy with training set sizes of >104 and
is half as accurate as MPNN at training set sizes nearing 105 entries.

of 0.44 kcal/mol2.3× better than the 1.0 kcal/mol MAE of
the linear regression model. The increased accuracy of MPNN
can be traced to the limited complexity available to the linear
regression model. The LASSO model only includes 1228
parameters, far below the 897,409 parameters in the MPNN
model, which likely explains why the LASSO model does not
improve in accuracy when provided with more than 104 data
points. Some routes to improving the accuracy of this linear
regression model could be to add non-linear combinations of
existing features or to add more features by diﬀerentiating
functional groups based on second-nearest neighbors. The
elegance of the MPNN is that it learns such non-linear
combinations and more complex features automatically. As we
demonstrate here, the ability to learn such interactions leads to
superior predictive accuracy.
We studied the outliers of the MPNN to gain a further
insight into the generalizability of the model. The compounds
with the largest absolute errors are those with particularly high

Figure 4. (a) Predictions of solvation energy as a function of the dielectric constant of a solvent for four solute molecules with diﬀerent solvation
behaviors. Our MPNN model includes the dielectric constant of the solvent as an input feature. It was trained on the solvation energies of 117,232
molecules. The solid blue line is the solvation energy prediction for each molecule from our MPNN. Black circles represent solvents that were
included in the training set (left to right: acetone, ethanol, DMSO, and water) and the red square is a solvent withheld from our training set
(ACN). (b) Performance of the MPNN model in predicting the solvation energy in DMSO for all 130,258 molecules in our dataset.
5993

https://doi.org/10.1021/acs.jpca.1c01960
J. Phys. Chem. A 2021, 125, 5990−5998



The Journal of Physical Chemistry A

pubs.acs.org/JPCA

Article

Figure 5. Performance of two diﬀerent MPNN models trained on 117,232 molecules with nine or fewer non-hydrogen atoms on predicting the
solvation energy in ACN, DMSO, and water on our validation holdout sets: (a,c) a model which learns the solvation energy from a molecular
ﬁngerprint and (b,d) a model which predicts solvation energy by summing over contributions from each atom. (a,b) Histogram of errors in the
solvation energy in the ACN solvent for holdout molecules from the QM9 dataset. (c,d) The MAE for molecules from the QM9 holdout set and
our larger molecule holdout set binned by the number of non-hydrogen atoms. Error bars represent the standard error of the mean.

molecules with 14 heavy atoms. The increase in error with
molecule size is not monotonic, with molecules of size 11
having errors around 2× smaller than errors for molecules with
14 non-hydrogen atoms but is strong enough that we speculate
that the poor performance of our models on the Pedley test set
has to do with the issues with capturing the eﬀect of molecular
size more than the presence of structural motifs absent from
the training set.
We explored improving the performance of our model by
adopting an “atomic contribution” model for solvation energy.
As illustrated in Figure 1, our current model generates a single
ﬁngerprint to describe an entire molecule by summing the
ﬁngerprint of each atom (the output of the message passing
layers) and computing the solvation energy from the resultant
“molecular ﬁngerprint”. An alternative approach is to predict
“atomic contributions” for each atom in the molecule
separately and then to express the molecular property as a
sum of these contributionsa technique used often in ML
models of molecular energy.16,31,40,41 The atomic contribution
approach explicitly encodes a linear relationship between a
molecular property and the size of the molecule, which is a fair
approximation for solvation energies given the known
correlation between solvation energy and molecular size.14
The atomic contribution model performs worse for the
QM9 holdout set but has a less dramatic increase in error with
molecule size than our original model. As shown in Figure 5b,
the atomic contribution model has a 50% larger error on the
QM9 dataset, 0.47 kcal/mol, than the molecular ﬁngerprint
model, 0.32 kcal/mol. However, we see a much smoother
increase with error with increasing molecular size. The MAE
for molecules with 14 non-hydrogen atoms is, for example, at
least 5× smaller than our molecular ﬁngerprint model. The
drastic diﬀerences between the performance on QM9 and our
external test sets strongly motivates the need for evaluating ML
models more closely to how they will be used for larger
molecules and the need for molecular datasets with larger
molecules. We also note that the atomic contribution model is
not fully free from the eﬀect of molecular size on error. The
errors steadily increase for molecules with sizes more distant
from nine non-hydrogen atoms, which is the size with the

The MAE of our dielectric constant model for predicting
solvation energy in water is not changed by the exclusion of
ACN training data; the MAE decreases by only 5 × 10−4 kcal/
mol. Based on these results, we see neither beneﬁt nor
disadvantage to training a ML model on data from more
solvents at least for solvents that exist within the range of
dielectric constants in our training set.
The fact that the dielectric constant model uses solvent
properties as inputs allows us to predict the properties of
solvents not contained within the training set. As shown in
Figure 4, our model smoothly interpolates how the solvation
energy changes as a function of dielectric constant. The model
also appropriately detects that the solvation energy for propane
increases for solvents with large dielectric constants and
increases for water. The dielectric constant model trained
without any data with ACN as a solvent achieves an MAE for
an ACN solvation energy of 0.62 kcal/mol across all molecules
and an MAE of 0.70 kcal/mol on molecules from the holdout
set, for which neither molecule nor solvent are included in the
training set. From this, we conclude that the model is able to
interpolate between solvents with dielectric constants between
10 and 80ϵ0.
Improving Performance for Molecules Larger than in
the Training Set. The training data used in the previous
sections are drawn from the same population, which provides a
useful but limited view of the utility of our model. Speciﬁcally,
all molecules were taken from the QM9 dataset and, as a result,
have only nine or fewer non-hydrogen atoms, limiting the
estimates of accuracy discussed above to only these smaller
molecules. We therefore studied the accuracy of our model on
predicting the solvation energy of 191 molecules with more
than nine non-hydrogen atoms (i.e., our Pedley test set).
The errors for these molecules from the Pedley test set are
substantially greater than those observed for the QM9-based
test set. For example, the MAE for solvation energy in ACN for
the Pedley test set is 3.46 kcal/molover 10× larger than the
MAE of 0.32 kcal/mol for the QM9 test set. The errors tend to
increase with molecule size. As shown in Figure 5c, the MAE
for molecules in the Pedley test set generally increases as a
function of molecular size and reaches over 10 kcal/mol for
5994

https://doi.org/10.1021/acs.jpca.1c01960
J. Phys. Chem. A 2021, 125, 5990−5998



The Journal of Physical Chemistry A

pubs.acs.org/JPCA

Article

Figure 6. Comparison of diﬀerent domains of applicability metrics: Tanimoto similarity to the 64 most similar molecules in the training set,
distance based on the atomic and molecular ﬁngerprints from an MPNN model for solvation energy, and variance in a bootstrap ensemble of 16
models. The performance of each metric was assessed using 13,217 molecules from our holdout set using ACN as a solvent. (a) MAE and
normalized applicability metric for molecules divided into 16 bins grouped by each metric. The metrics are normalized by applying a linear
transformation such that the 1st and 99th percentiles have values of 0 and 1, respectively. (b) Likelihood of an error greater than 1 kcal/mol for 16
bins of molecules grouped by each metric. The dotted lines between data points are intended to guide the eye. The black dashed lines show the (a)
MAE on the entire holdout set and (b) fraction of errors >1 kcal/mol for the holdout set.

poor performance. Such behavior suggests for us to consider
metrics that measure similarity in functional groups between
molecules to estimate the performance of our model.
Our metrics are based on common approaches from the
chemoinformatics community,42 such as distances from the
training set and variance of an ensemble of ML models. We
explore three diﬀerent applicability metrics. We ﬁrst chose the
Tanimoto similarity based on Morgan Fingerprints,43 which
represents a molecular similarity metric that is not tuned for
any particular property. We also chose two applicability metrics
based on our MPNN models. The ﬁrst is based on the
molecular ﬁngerprint MPNN model, where we compute the
mean L2 distance between the molecular ﬁngerprint (see
Figure 1) for a molecule and those of the 64 closest molecules
from the training set. The second is based on the ﬁngerprints
for each atom as computed using the atomic contribution
model, where we compute the dissimilarity of a molecule from
one in the training set as the mean distance between the most
similar pairs of atoms

greatest amount of training data. There could be a few
explanations for the relationship between error and molecule
size. Namely, there could be an eﬀect of bias in the training set
due to the overrepresentation of data with nine heavy atoms or
a failure in the assumption that the atom count and solvation
energy should be linearly related. We therefore propose reweighting techniques to account for training set bias or
diﬀerent approaches for summing atomic contributions (e.g.,
generalized means) as a direction for modeling non-linear
relationships between atomic size and property.
The ﬁnal ingredient of the utility of our model is the speed
at which we can evaluate new molecules. Evaluating the model
requires parsing the SMILES string to an RDKit molecule
object, converting the RDKit molecule to a graph representation compatible with TensorFlow and evaluating the MPNN.
The conversion steps can be performed in parallel for each
molecule and the individual mathematical operations of the
MPNN can also be parallelized, allowing us to exploit highly
parallel CPUs and general purpose GPUs. Accordingly, we can
achieve an evaluation rate of 103 molecules per second on two
32-core Intel Xeon CPU E5-2683 v4 CPUs. This evaluation
rate makes it possible to evaluate all of QM9 (105 molecules)
in 2 min and all of ZINC15 in 33 h using only a single
workstation. The extreme evaluation rates noted here can be
further improved by running the models on multiple nodes of a
supercomputer, making it possible to rapidly screen molecule
spaces with billions of candidates.
Quantifying Domain of Applicability. The empirical
nature of ML models makes it diﬃcult to identify which
predictions are most or least likely to be accurate. In the
previous section, we established that molecular size correlates
with errora likely eﬀect of the training set being biased
toward molecules with nine non-hydrogen atoms. Here, we
explore improved metrics for establishing a quantitative
estimate of the size of the error for a prediction.
Our previous analysis of the performance of the models on
predicting solvation energies in water give clues on what is
important in estimating the performance of the model. We
noted, in particular, that zwitterions were overrepresented in
the molecules with the largest error (48% of largest errors were
zwitterions compared to only 0.4% in the test set). The poor
performance of the models on a class of molecules that are
structurally distinct from most of the training set is reﬂective
on how the MPNN models implicitly learn the behavior of
functional groups within molecules. Unseen or rare functional
groups will have little training data, which leads can lead to

a(m , n) =

1
mi − nj
∑ max
2
j
Nm i

(2)

where m is the molecule, n is a molecule from the training set,
Nm is the number of atoms in m, the sum ∑i is over all atoms
in m, the maximum max is over all atoms in n, mi is the atomic
j

ﬁngerprint of atom i in m, nh is the ﬁngerprint of atom j in n,
and ||···||2 represents the L2 norm. We note that a(m, n) is not a
proper distance metric, as a(m, n) ≠ a(n, m). As with the
“molecular ﬁngerprint distance”, we compute the atomic
ﬁngerprint distance as the average distance over the 64 closest
molecules. In all cases, we assume that larger distances or lower
similarities from the training set indicate greater likelihood that
a molecule is outside of the training set.
Our ﬁnal applicability metric is based on the variance of the
predictions of an ensemble of MPNNs. We created an
ensemble by training 16 atomic contribution MPNNs each
with a diﬀerent, randomly selected subset created by randomly
sampling the full training set with replacement (i.e., a
bootstrapped ensemble). The variance of this training captures
the uncertainty of individual predictions.44,45 We assume that
molecules with larger uncertainties are more likely to have
larger errors.
We evaluated the quality of our domain of applicability
metrics by measuring the correlation between each application
metric for each prediction and the size of the observed error.
5995

https://doi.org/10.1021/acs.jpca.1c01960
J. Phys. Chem. A 2021, 125, 5990−5998



The Journal of Physical Chemistry A

pubs.acs.org/JPCA

We seek applicability metrics for which molecules with larger
applicability metrics have a greater likelihood of having larger
errors than those with smaller metrics. The statistical variation
in error means that we assume that these changes will be
visible in averages and not that all points with larger metrics
will have larger errors. Accordingly, we grouped our data into
16 bins based on each of the applicability metric and show the
average errors and likelihood of an error being greater than 1
kcal/mol. In all cases, we used the predicted solvation energy
in ACN based on the atomic contribution MPNNs for
molecules in our Holdout and Pedley test sets. The results are
shown in Figure 6.
We ﬁnd that the applicability based on the molecular
ﬁngerprint MPNN yields the best performance according to
both quality metrics. We ﬁnd that there is the strongest
correlation between both the magnitude of error and the
likelihood of a 1 kcal/mol error. The MPNN-based ﬁngerprint
distance strongly outperforms a generic circular ﬁngerprint
with Tanimoto similarity, suggesting a beneﬁt to using
ﬁngerprints trained to reﬂect a certain atomic property as a
similarity metric. In general, we ﬁnd a signiﬁcant need for
developing a good domain of applicability metrics. As
illustrated by specialized MPNN ﬁngerprints outperforming
conventional Tanimoto ﬁngerprints, the dependence of model
error on inputs can require complicated models. Further
research on developing complex applicability metrics or, even,
measuring the performance of diﬀerent metrics (e.g., ref 46) is
greatly needed.
We therefore train a linear regression and logistic
classiﬁcation model on the MPNN molecular ﬁngerprint
distance as a means to provide a quantitative estimate of the
uncertainty of any prediction made using our model. Users of
the model can deﬁne their own cutoﬀ for a domain of
applicability depending on how much uncertainty that can
tolerate. We can also use the uncertainties of the models to
further improve them47 or accelerate the discovery of
molecules with target properties.48,49

■

Detailed description of the message-passing neural
network architecture (PDF)

AUTHOR INFORMATION

Corresponding Author

Logan Ward − Data Science and Learning Division, Argonne
National Laboratory, Lemont, Illinois 60439, United States;
orcid.org/0000-0002-1323-5939; Email: lward@anl.gov
Authors

Naveen Dandu − Materials Science Division, Argonne
National Laboratory, Lemont, Illinois 60439, United States;
orcid.org/0000-0001-7122-8537
Ben Blaiszik − Data Science and Learning Division, Argonne
National Laboratory, Lemont, Illinois 60439, United States;
Globus, University of Chicago, Chicago, Illinois 60637,
United States
Badri Narayanan − Materials Science Division, Argonne
National Laboratory, Lemont, Illinois 60439, United States;
Department of Mechanical Engineering, University of
Louisville, Louisville, Kentucky 40292, United States
Rajeev S. Assary − Materials Science Division, Argonne
National Laboratory, Lemont, Illinois 60439, United States;
orcid.org/0000-0002-9571-3307
Paul C. Redfern − Materials Science Division, Argonne
National Laboratory, Lemont, Illinois 60439, United States
Ian Foster − Data Science and Learning Division, Argonne
National Laboratory, Lemont, Illinois 60439, United States;
Department of Computer Science and Globus, University of
Chicago, Chicago, Illinois 60637, United States
Larry A. Curtiss − Materials Science Division, Argonne
National Laboratory, Lemont, Illinois 60439, United States;
orcid.org/0000-0001-8855-8006
Complete contact information is available at:
https://pubs.acs.org/10.1021/acs.jpca.1c01960
Notes

■

The authors declare no competing ﬁnancial interest.
The data, models, and code used in this study are freely
available via the Materials Data Facility,50,51 DLHub,22,50 and
GitHub, respectively. Speciﬁcally, we have published our
training and test datasets,20 the best trained models (the
atomic contribution models with solvent dependence), and the
code used for training and testing our ML models.21

CONCLUSIONS
We have presented QM9-solvation, a new dataset of DFTcomputed solvation energies of 130,258 molecules in ﬁve
solvents, a total of 651,290 values, and used this new dataset to
train a suite of ML models. Our best-performing ML models,
based on a message-passing neural network that takes
properties of both solute molecules and solvent as inputs,
achieve an accuracy of 0.5 kcal/mol, 2× better than group
contribution models. We established that our models can
generalize to predict solvation energy in solvents outside of the
training set and for molecules larger than those used to train
our datasets. Finally, we created a domain-of-applicability
metric to provide users an estimate of whether the predictions
of our models are trustworthy for a certain molecule. The data,
models, and code used in this study are all freely available
online, and the ML models are accessible through a simple web
interface that both invokes the model and provides estimates of
model uncertainty for each prediction.

■

Article

■

ACKNOWLEDGMENTS
L.W., N.D., B.N., R.S.A., P.C.R., and L.A.C. were supported to
develop the database and ML algorithms as part of the Joint
Center for Energy Storage Research (JCESR), an Energy
Innovation Hub funded by the US Department of Energy,
Oﬃce of Science, Basic Energy Sciences. L.W. and I.F. were
supported to perform ML tasks on High-Performance
Computing by the DOE Exascale Computing Project,
ExaLearn Co-design Center. B.B. and work on making the
dataset openly available was performed under ﬁnancial
assistance award 70NANB14H012 from the U.S. Department
of Commerce, National Institute of Standards and Technology,
as part of the Center for Hierarchical Material Design
(CHiMaD). B.B. and I.F. were also supported by the National
Science Foundation as part of the Midwest Big Data Hub
under NSF award number: 1636950 “BD Spokes: SPOKE:
MIDWEST: Collaborative: Integrative Materials Design

ASSOCIATED CONTENT

* Supporting Information
sı

The Supporting Information is available free of charge at
https://pubs.acs.org/doi/10.1021/acs.jpca.1c01960.
5996

https://doi.org/10.1021/acs.jpca.1c01960
J. Phys. Chem. A 2021, 125, 5990−5998



The Journal of Physical Chemistry A

pubs.acs.org/JPCA

(19) Marenich, A. V.; Cramer, C. J.; Truhlar, D. G. Universal
Solvation Model Based on Solute Electron Density and on a
Continuum Model of the Solvent Defined by the Bulk Dielectric
Constant and Atomic Surface Tensions. J. Phys. Chem. B 2009, 113,
6378−6396.
(20) Ward, L.; Dandu, N.; Blaiszik, B.; Narayanan, B.; Assary, R. S.;
Redfern, P. C.; Foster, I.; Curtiss, L. A. Dataset: Datasets and Machine
Learning Models for Accurate Estimates of Solvation Energy in Multiple
Solvents; Materials Data Facility, 2021. https://petreldata.net/mdf/
detail/solv_ml_v1.2.
(21) https://github.com/globus-labs/solvation-energy-ml. (accessed
12 Feb 2021).
(22) Chard, R.; Li, Z.; Chard, K.; Ward, L.; Babuji, Y.; Woodard, A.;
Tuecke, S.; Blaiszik, B.; Franklin, M.; Foster, I. DLHub: Model and
data serving for science. International Parallel and Distributed
Processing Symposium, 2019; pp 283−292.
(23) Frisch, M. J.; et al. Gaussian 16, Revision C.01; Gaussian Inc.:
Wallingford CT, 2016.
(24) Ramakrishnan, R.; Dral, P. O.; Rupp, M.; von Lilienfeld, O. A.
Quantum chemistry structures and properties of 134 kilo molecules.
Sci. Data 2014, 1, 140022.
(25) Narayanan, B.; Redfern, P. C.; Assary, R. S.; Curtiss, L. A.
Accurate quantum chemical energies for 133 000 organic molecules.
Chem. Sci. 2019, 10, 7449−7455.
(26) Narayanan, B.; Redfern, P.; Ward, L.; Blaiszik, B.; Foster, I.;
Assary, R. S.; Curtiss, L. G4MP2-GDB9 Database, 2019. https://
petreldata.net/mdf/detail/narayananbadri_g4mp2gdb9_database_v1.
1.
(27) Dandu, N.; Ward, L.; Assary, R. S.; Redfern, P. C.; Narayanan,
B.; Foster, I. T.; Curtiss, L. A. Quantum-Chemically Informed
Machine Learning: Prediction of Energies of Organic Molecules with
10 to 14 Non-hydrogen Atoms. J. Phys. Chem. A 2020, 124, 5804−
5811.
(28) Pedley, J. Thermochemical Data and Structures of Organic
Compounds; CRC Press, 1994; Vol. 1.
(29) Chen, C.; Ye, W.; Zuo, Y.; Zheng, C.; Ong, S. P. Graph
networks as a universal machine learning framework for molecules
and crystals. Chem. Mater. 2019, 31, 3564−3572.
(30) St. John, P. C.; Kemper, T. W.; Wilson, A. N.; Guan, Y.;
Crowley, M. F.; Nimlos, M. R.; Larsen, R. E.; Larsen, R. E. Messagepassing neural networks for high-throughput polymer screening. J.
Chem. Phys. 2019, 150, 234111.
(31) Faber, F. A.; Christensen, A. S.; Huang, B.; von Lilienfeld, O. A.
Alchemical and structural distribution based representation for
universal quantum machine learning. J. Chem. Phys. 2018, 148,
241717.
(32) Anatole von Lilienfeld, O.; Burke, K. Retrospective on a decade
of machine learning for chemical discovery. Nat. Commun. 2020, 11,
4895.
(33) Blum, L. C.; Reymond, J.-L. 970 million druglike small
molecules for virtual screening in the chemical universe database
GDB-13. J. Am. Chem. Soc. 2009, 131, 8732−8733.
(34) Zhang, J.; Zhang, H.; Wu, T.; Wang, Q.; van der Spoel, D.
Comparison of Implicit and Explicit Solvent Models for the
Calculation of Solvation Free Energy in Organic Solvents. J. Chem.
Theory Comput. 2017, 13, 1034−1043.
(35) Chen, J.; Shao, Y.; Ho, J. Are Explicit Solvent Models More
Accurate than Implicit Solvent Models? A Case Study on the
Menschutkin Reaction. J. Phys. Chem. A 2019, 123, 5580−5589.
(36) Shin, S.-H.; Yun, S.-H.; Moon, S.-H. A review of current
developments in non-aqueous redox flow batteries: characterization of
their membranes for design perspective. RSC Adv. 2013, 3, 9095.
(37) Ward, L.; Blaiszik, B.; Foster, I.; Assary, R. S.; Narayanan, B.;
Curtiss, L. Machine learning prediction of accurate atomization
energies of organic molecules from low-fidelity quantum chemical
calculations. MRS Commun. 2019, 9, 891−899.
(38) Wildman, S. A.; Crippen, G. M. Prediction of physicochemical
parameters by atomic contributions. J. Chem. Inf. Comput. Sci. 1999,
39, 868−873.

(IMaD): Leverage, Innovate, and Disseminate.” Work by B.B.
on making ML models available through DLHub was
supported in part by Laboratory Directed Research and
Development funding from Argonne National Laboratory
under U.S. Department of Energy under contract DE-AC0206CH11357. We also thank the Argonne Leadership
Computing Facility for access to the PetrelKube Kubernetes
cluster. This research used resources of the Argonne
Leadership Computing Facility, a DOE Oﬃce of Science
User Facility supported under contract DE-AC02-06CH11357.

■

Article

REFERENCES

(1) Cheng, L.; Assary, R. S.; Qu, X.; Jain, A.; Ong, S. P.; Rajput, N.
N.; Persson, K.; Curtiss, L. A. Accelerating Electrolyte Discovery for
Energy Storage with High-Throughput Screening. J. Phys. Chem. Lett.
2015, 6, 283−291.
(2) Mennucci, B. Continuum Solvation Models: What Else Can We
Learn from Them? J. Phys. Chem. Lett. 2010, 1, 1666−1674.
(3) Sterling, T.; Irwin, J. J. ZINC 15 − Ligand Discovery for
Everyone. J. Chem. Inf. Model. 2015, 55, 2324−2337.
(4) Saadi, A. A.; et al. IMPECCABLE: Integrated Modeling PipelinE
for COVID Cure by Assessing Better LEads. 2020, arXiv:2010.06574.
(5) Ruddigkeit, L.; van Deursen, R.; Blum, L. C.; Reymond, J.-L.
Enumeration of 166 Billion Organic Small Molecules in the Chemical
Universe Database GDB-17. J. Chem. Inf. Model. 2012, 52, 2864−
2875.
(6) Babuji, Y.; et al. Targeting SARS-CoV-2 with AI- and HPCenabled Lead Generation: A First Data Release, 2020,
arXiv:2006.02431
(7) Le, T.; Epa, V. C.; Burden, F. R.; Winkler, D. A. Quantitative
Structure−Property Relationship Modeling of Diverse Materials
Properties. Chem. Rev. 2012, 112, 2889−2919.
(8) Mobley, D. L.; Guthrie, J. P. FreeSolv: A database of
experimental and calculated hydration free energies, with input files.
J. Comput.-Aided Mol. Des. 2014, 28, 711−720.
(9) Goh, G. B.; Siegel, C.; Vishnu, A.; Hodas, N. O.; Baker, N.
Chemception: A deep neural network with minimal chemistry
knowledge matches the performance of expert-developed QSAR/
QSPR models. 2017, arXiv:1706.06689, arXiv preprint.
(10) Wu, Z.; Ramsundar, B.; Feinberg, E. N.; Gomes, J.; Geniesse,
C.; Pappu, A. S.; Leswing, K.; Pande, V. MoleculeNet: a benchmark
for molecular machine learning. Chem. Sci. 2018, 9, 513−530.
(11) Rauer, C.; Bereau, T. Hydration free energies from kernel-based
machine learning: Compound-database bias. J. Chem. Phys. 2020, 153,
014101.
(12) Riniker, S. Molecular Dynamics Fingerprints (MDFP):
Machine Learning from MD Data To Predict Free-Energy Differences. J. Chem. Inf. Model. 2017, 57, 726−741.
(13) Marenich, A. V.; Kelly, C. P.; Thompson, J. D.; Hawkins, G. D.;
Chambers, C. C.; Giesen, D. J.; Winget, P.; Cramer, C. J.; Truhlar, D.
G. Minnesota Solvation Database, version, 2012.
(14) Borhani, T. N.; García-Muñoz, S.; Vanesa Luciani, C.; Galindo,
A.; Adjiman, C. S. Hybrid QSPR models for the prediction of the free
energy of solvation of organic solute/solvent pairs. Phys. Chem. Chem.
Phys. 2019, 21, 13706−13720.
(15) Subramanian, V.; Ratkova, E.; Palmer, D.; Engkvist, O.;
Fedorov, M.; Llinas, A. Multi-solvent models for solvation free energy
predictions using 3D-RISM Hydration Thermodynamic Descriptors.
J. Chem. Inf. Model. 2020, 60, 2977.
(16) Schütt, K. T.; Sauceda, H. E.; Kindermans, P.-J.; Tkatchenko,
A.; Müller, K.-R. SchNet - A deep learning architecture for molecules
and materials. J. Chem. Phys. 2018, 148, 241722.
(17) Gilmer, J.; Schoenholz, S. S.; Riley, P. F.; Vinyals, O.; Dahl, G.
E. Neural message passing for quantum chemistry. 34th International
Conference on Machine Learning, 2017; pp 2053−2070.
(18) Smith, J. S.; Isayev, O.; Roitberg, A. E. ANI-1: an extensible
neural network potential with DFT accuracy at force field computational cost. Chem. Sci. 2017, 8, 3192−3203.
5997

https://doi.org/10.1021/acs.jpca.1c01960
J. Phys. Chem. A 2021, 125, 5990−5998



The Journal of Physical Chemistry A

pubs.acs.org/JPCA

Article

(39) Tibshirani, R. Regression Shrinkage and Selection Via the
Lasso. J. Roy. Stat. Soc. B 1996, 58, 267−288.
(40) Behler, J.; Parrinello, M. Generalized neural-network
representation of high-dimensional potential-energy surfaces. Phys.
Rev. Lett. 2007, 98, 146401.
(41) Bartók, A. P.; Payne, M. C.; Kondor, R.; Csányi, G. Gaussian
Approximation Potentials: The Accuracy of Quantum Mechanics,
without the Electrons. Phys. Rev. Lett. 2010, 104, 136403.
(42) Klingspohn, W.; Mathea, M.; ter Laak, A.; Heinrich, N.;
Baumann, K. Efficiency of different measures for defining the
applicability domain of classification models. J. Cheminf. 2017, 9, 44.
(43) Rogers, D.; Hahn, M. Extended-connectivity fingerprints. J.
Chem. Inf. Model. 2010, 50, 742−754.
(44) Ling, J.; Hutchinson, M.; Antono, E.; Paradiso, S.; Meredig, B.
High-dimensional materials and process optimization using datadriven experimental design with well-calibrated uncertainty estimates.
Integr. Mater. Manuf. Innov. 2017, 6, 207−217.
(45) Peterson, A. A.; Christensen, R.; Khorshidi, A. Addressing
uncertainty in atomistic machine learning. Phys. Chem. Chem. Phys.
2017, 19, 10978−10985.
(46) Tran, K.; Neiswanger, W.; Yoon, J.; Zhang, Q.; Xing, E.; Ulissi,
Z. W. Methods for comparing uncertainty quantifications for material
property predictions. Mach. learn.: sci. technol. 2020, 1, 025006.
(47) Gubaev, K.; Podryabinkin, E. V.; Shapeev, A. V. Machine
learning of molecular properties: Locality and active learning. J. Chem.
Phys. 2018, 148, 241727.
(48) Lookman, T.; Balachandran, P. V.; Xue, D.; Yuan, R. Active
learning in materials science with emphasis on adaptive sampling
using uncertainties for targeted design. npj Comput. Mater. 2019, 5,
21.
(49) Doan, H. A.; Agarwal, G.; Qian, H.; Counihan, M. J.;
Rodríguez-López, J.; Moore, J. S.; Assary, R. S. Quantum ChemistryInformed Active Learning to Accelerate the Design and Discovery of
Sustainable Energy Storage Materials. Chem. Mater. 2020, 32, 6338−
6346.
(50) Blaiszik, B.; Ward, L.; Schwarting, M.; Gaff, J.; Chard, R.; Pike,
D.; Chard, K.; Foster, I. A data ecosystem to support machine
learning in materials science. MRS Commun. 2019, 9, 1125−1133.
(51) Blaiszik, B.; Chard, K.; Pruyne, J.; Ananthakrishnan, R.; Tuecke,
S.; Foster, I. The Materials Data Facility: Data services to advance
materials science research. JOM 2016, 68, 2045−2052.

5998

https://doi.org/10.1021/acs.jpca.1c01960
J. Phys. Chem. A 2021, 125, 5990−5998

